\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
%\usepackage{longtable}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{courier}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mdwlist} % tighter description environment (starred)
\usepackage{gensymb}
\usepackage{hyperref}
\usepackage{fp}
\usepackage{longtable}
\usepackage{sparklines}
\usepackage{caption}

%% \usepackage{showframe}  %% Use this for debugging margin overruns in tables.
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{softdev}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{xspace}
\usepackage{pdflscape}
\usepackage{sparklines}
\usepackage{float}
\usepackage{siunitx}


\newcommand{\krun}{\textsc{Krun}\xspace}
\newcommand{\graalce}{\textsc{Graal CE}\xspace}
\newcommand{\graalcehs}{\textsc{Graal CE Hotspot}\xspace}
\newcommand{\jnine}{\textsc{OpenJ9}\xspace}
\newcommand{\bencherseven}{Linux$_\mathrm{1240v5}$\xspace}
\newcommand{\bencherten}{Linux$_\mathrm{1240v6}$\xspace}
\input{tables/preamble}

\begin{document}


\title{Analysing the Renaissance Benchmark Suite~\footnote{Updates to this paper will be found at \url{XXX}}}

\author{Edd Barrett and Laurence Tratt}

\maketitle

\begin{abstract}
\noindent In this early draft we evaluate the warmup characteristics of the
Renaissance Benchmark Suite.~\cite{prokopec19renaissance} We do so using the
benchmarking practices from our earlier work on VM
warmup~\cite{barrett16warmup}. Namely, we run the benchmarks for longer than is
typical and we use a controlled benchmarking environment enforced by our
benchmark runner, \krun. We find that many benchmarks, when run using Graal
Community Edition, Hotspot, or OpenJ9, exhibit inconsistency at
the process execution level, and that many either take a long time to
stabilise, or do not stabilise within 2000 iterations.
\end{abstract}

\section{Benchmarking Method}
\label{sec:eval}

Following the precedent set by~\cite{prokopec19renaissance} we ran the
Renaissance benchmarks using different \emph{VM configurations}. \graalce is
Graal Community Edition invoked with the default Graal compiler options, and
\graalcehs is the same binary, only invoked with arguments to disable the Graal
compiler (instead using the regular OpenJDK JIT compiler). We used version 0.9.0
of the renaissance suite and version 1.0.0-rc16 of the Graal Community
Edition JVM.

To see how a non-Oracle JVM fares on the same suite, we added a third VM
configuration, \jnine, which is version 0.15.1 of the OpenJ9 JVM.

Note that for some configurations, we had to skip certain benchmarks.
Specifically: we always skip the \emph{dummy} benchmark, as this does nothing;
we always skip \emph{finagle-chirper} and \emph{finagle-http} as these use the
loopback network interface, which we have instructed Krun to disable; we skip
\emph{db-shootout} on \jnine, as it throws a null pointer exception; we always
skip \emph{movie-lens}, as it exceeds the 18GiB heap we permitted; and finally
we skip \emph{page-rank} on \graalce and \graalcehs, due to crashes in the
benchmark's teardown code.

We adopted the benchmarking terminology used in~\cite{barrett16warmup}. A
\emph{process execution} is the execution of a single operating system process.
An \emph{in-process iteration} is a single iteration of a benchmark within a
process execution, and a single process execution executes many in-process
iterations. In our experiment, each VM configuration and benchmark pairing is
run using 10 process executions, and each process execution runs 2000
in-process iterations.

We ran the Renaissance benchmarks in a similar fashion to that used
in.~\cite{barrett16warmup} We use the \krun runner with
settings designed to mimimise (where possible) measurement variation introduced
by the benchmarking environment. For example, consistent stack and heap limits
are set, undesirable daemons are disabled during benchmarking, the system
temperature before each run is not allowed deviate too much, and network
interfaces are disabled during benchmarking.

One small change was made to \krun. For each process execution, usually a
\krun experiment will call an \emph{iterations runner} which (amongst other
things) allocates the results array in an appropriate manner and collects
wallclock times using a carefully selected high-resolution monotonic clock. Instead
of using an iterations runner, we changed \krun, adding the ability to defer
results collection to an external program.~\footnote{For details, see
\texttt{ExternalSuiteVMDef} in the \krun source code.} Although such an approach
means that we lose some of the advanced \krun functionality (e.g.
measuring core cycles and checking \texttt{APERF}/\texttt{MPERF} ratios), this
does allow us to re-use a large chunk of the benchmark running code from the
Renaissance suite.

We ran the experiments on two similar machines. The first, \bencherseven, is a
Dell PowerEdge R330 with an Intel Xeon E3-1240 v5 CPU, running at 3.50GHz and
with 24GiB of RAM. The second machine, \bencherten, is also a Dell PowerEdge
R330, but has an Intel Xeon E3-1240 v6 CPU running at 3.70GHz
and with 32Gib of RAM. Both machines run Debian-9.9 with an indentical set of
installed packages. Both machines have hyper-threading and turbo boost
disabled.

Note that because we ran the benchmarks and VMs unpatched, we inherit the
choice of clock used for measurements. In the Renaissance suite, benchmark
timings are collected using \texttt{System.nanoTime()}, whose behaviour is
platform dependent. On the Linux machines we used, \texttt{System.nanoTime()}
is implemented with a call to \texttt{clock\_gettime(2)} and using the
\texttt{CLOCK\_MONOTONIC} clock source.\footnote{This is a monotonic clock which
is unaffected by user changes to the time, but which \emph{is} affected by
adjustments by NTP. Ordinarily, on Linux machines we'd prefer the
\texttt{CLOCK\_MONOTONIC\_RAW} clock source.}

\vspace{1em}
The complete source code for our experiment can be found at:
\begin{center}
\url{https://github.com/softdevteam/renaissance_reproduction/}
\end{center}

\vspace{1em}
The data used in this paper is version 0.1, and can be downloaded from:
\begin{center}
\url{https://archive.org/download/softdev_renaissance_analysis/runs/v0.1/}
\end{center}



\section{Results}
\label{sec:results}

We ran our experiment as described in the previous section, giving us a total
of 1200 process executions and 2.4 million in-process iterations worth of
data. We present our results using the same metrics as proposed
in~\cite{barrett16warmup} and refer the reader to this publication for a more
in-depth description of the format of our results tables. In short, for each
machine, we present one result table. Each row in a table shows the results for
one VM configuration and benchmark pairing.

The first metric we report is the pairing's \emph{classification}. The
behaviour of each process execution is individually (but automatically) classified as either:
\emph{warmup} (\warmup); \emph{slowdown} (\slowdown); \emph{flat} (\flatc); or
\emph{no steady state} (\nosteadystate). Then based on these classifications,
we decide an overall summary classification for each VM configuration and benchmark
pairing.

For a \emph{consistent} classification summary result (where all process
executions behaved the same), a single classification symbol is reported,
whereas for \emph{inconsistent} results (where we observed more than a single
classification) the constituent classifications are shown with an indicator of
whether the inconsistency is a \emph{good inconsistency} (\goodinconsistent)
or a \emph{bad inconsistency} (\badinconsistent). For example
\badinconsistent(8\slowdown, 2\warmup) means ``bad inconsistent: 8 slowdowns
and 2 warmups''. Only inconsistent behaviours composed of flat and warmup
classifications are considered ``good inconsistent''. This is because
benchmarks which slow down or fail to stabilise are undesirable.

If a steady state is achieved, we report three additional summary statistics.
\emph{Steady iter (\#)} is the median in-process iteration number to reach the
steady state, and \emph{Steady iter (s)} is the median wall-clock time (since
the beginning of the process execution) to reach the steady state; for both
measures we report the 5th and 95th percentiles in brackets below. \emph{Steady perf (s)}
is the mean steady state performance across all process executions (reported
with 99\% confidence intervals). Thumbnail histograms show the spread of
values; the red bars indicates the median values.

\subsection{Analysis}

The results for the two machines are shown in
Tables~\ref{tab:b7graalce}--\ref{tab:b10j9}.

\subsection{Graal Results}

The first thing we notice is that the steady state performances
of \graalce and \graalcehs (where available) are not vastly different. The results
presented in the original Renaissance paper~\cite{prokopec19renaissance} showed
that Graal could achieve some substantial speedups compared to Hotspot. Namely
a benchmark called \emph{factorie} achieved a speedup of about 3x, scrabble
about 2x, and \emph{naive-bayes} about 1.75x. In the version of the suite we
used, \emph{factorie} is absent, \emph{scrabble} achieved about a $15-20\%$
speedup and \emph{naive-bayes} couldn't be compared because it
didn't always stabilise. We acknowledge that this comparison isn't entirely
fair as it is unlikely that we are using the same versions of the benchmarks
and of Graal as the original paper (which does not specify which versions were
used). At the time of writing, the Renaissance website shows updated plots
showing results much more in keeping with our own.

Next we note that the benchmark classifications are broadly the same between
the two different machines. This not unexpected given that the hardware is
similar (although not identical). If anything, it seems that the
classifications are more similar for \graalce than for \graalcehs, perhaps
suggesting that \graalce is a bit more \emph{machine deterministic} (however
the deviations are not large).

The next observation worth note is that around half (44/80) of the VM
configuration and benchmarking pairings are classified as "bad inconsistent''.
Thus the Renaissance suite when run with Graal exhibits \emph{process execution non-determinism}. The histograms for \emph{Steady iter \#}, \emph{Steady
iter (s)} and \emph{Steady perf (s)} also confirm this: the distributions are rarely
normal, and are often bi-modal. It is not clear, however, what the sources of
this variance are (it could be the operating system, the hardware, the VM, etc.).

Finally, we highlight the tendency for some benchmarks to stabilise very late
or not at all. This is troublesome since few current benchmarking approaches
account for such cases, simply assuming that benchmarks always stabilise after
a fixed number of in-process iterations. This can lead to inaccurate results,
and in turn to false conclusions.

\subsection{OpenJ9 Results}

Again, the \jnine results are broadly similar between the two machines: many
benchmarks have inconsistent behaviour, and the time taken to achieve a steady
state is also highly variable with many benchmarks sometimes taking hundreds or
thousands of in-process iterations to stabilise. Some never stabilise.

Although it would be interesting to compare the \jnine results with that of \graalce,
a proper comparison is hindered by the fact that many rows are incomparable due
to either benchmarks being skipped on one or other VM, bad inconsistencies, or
overlapping confidence intervals (e.g. the future-genetic benchmark).

Of the benchmarks consistently comparable on both machines, we have:
\emph{chi-square}; \emph{fj-kmeans}; \emph{scala-stm-bench7}; and
\emph{scrabble}. All of these benchmarks reach a faster steady state on
\graalce (than on \jnine) by 25--45$\%$.


\bibliographystyle{plain}
\bibliography{bib}

\appendix

\newpage
\section{Results Tables}

\newcommand{\captionbsevengraalce}{Results for \graalce on \bencherseven.}
\input{tables/b7-graal-ce}

\newpage
\newcommand{\captionbsevengraalcehs}{Results for \graalcehs on \bencherseven.}
\input{tables/b7-graal-ce-hotspot}

\newpage
\newcommand{\captionbsevenjnine}{Results for \jnine on \bencherseven.}
\input{tables/b7-j9}

\newpage
\newcommand{\captionbtengraalce}{Results for \graalce on \bencherten.}
\input{tables/b10-graal-ce}

\newpage
\newcommand{\captionbtengraalcehs}{Results for \graalcehs on \bencherten.}
\input{tables/b10-graal-ce-hotspot}

\newpage
\newcommand{\captionbtenjnine}{Results for \jnine on \bencherten.}
\input{tables/b10-j9}

\section{Interesting Plots}

\includegraphics[width=.45\textwidth]{plots/slowdown1.pdf}
\includegraphics[width=.45\textwidth]{plots/no-steady1.pdf}\\
\includegraphics[width=.45\textwidth]{plots/no-steady2.pdf}
\includegraphics[width=.45\textwidth]{plots/cycles1.pdf}\\
\includegraphics[width=.45\textwidth]{plots/outliers1.pdf}
\includegraphics[width=.45\textwidth]{plots/fastearly1.pdf}\\
\includegraphics[width=.45\textwidth]{plots/steps1.pdf}
\includegraphics[width=.45\textwidth]{plots/leak.pdf}

\end{document}
